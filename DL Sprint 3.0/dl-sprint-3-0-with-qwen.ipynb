{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86200,"databundleVersionId":9773555,"sourceType":"competition"},{"sourceId":8218776,"sourceType":"datasetVersion","datasetId":4871830},{"sourceId":8300737,"sourceType":"datasetVersion","datasetId":4746046}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":27122.71615,"end_time":"2024-10-31T22:39:26.986644","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-31T15:07:24.270494","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0031255ec1ae417fa62c0b557a17919b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"006c3f3672dd4a9d8d32cacf4a7acd30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0a6f4784a699413aa69085ab65f51e85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7dfdf16ced044e2fb391d9537ed37b84","max":1671853,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eeb6831d4f3d48f99c86977a9395b71e","value":1671853}},"112fecebc1704c859b625df453613959":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0e52f7a5df94245b351931c29377e26","max":7031645,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a9cfa1095dd64c369985f659cb46797a","value":7031645}},"26c534ae4a444e2e83d0cef45939df19":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2768df3fa7f7455bbbb5739c589364e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a98b6e3d4a34fc6b87c608e6fd30951":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f4da549b3124f27855d69f86c17246e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3984a8c76c39459eae5e7daf3e1ec2e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a25ffb12264d477dae13999651d9d086","placeholder":"​","style":"IPY_MODEL_81b1b70022f94a88865eb1952049a3b9","value":" 7.30k/7.30k [00:00&lt;00:00, 540kB/s]"}},"3bbac326ed4a4b93a52adaf1adaded10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d72517155bc4db28b9029b91e6c2aa2","placeholder":"​","style":"IPY_MODEL_be369ccb0b1d444e898e09120a83423e","value":"tokenizer_config.json: 100%"}},"3ca2ab410742422098ee555c9bca210e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26c534ae4a444e2e83d0cef45939df19","placeholder":"​","style":"IPY_MODEL_bd8bba82c60c4bee99364af438e44b88","value":" 2.78M/2.78M [00:00&lt;00:00, 20.0MB/s]"}},"4d72517155bc4db28b9029b91e6c2aa2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4df89cbb549c4357aaa911378c1bdaa2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4e5d6a45d484737b1eb98ca870c10b5","placeholder":"​","style":"IPY_MODEL_7c230e2834804a8eafa1f2812212f786","value":"vocab.json: 100%"}},"527aed383aa447b0954c9f2d9afce42e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5533b8861dde41e0b9dfd8430c3f81f7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5533c15b76b3490e9bb969903c8507ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7da557a6bce646a896e590bc7df89736","placeholder":"​","style":"IPY_MODEL_9aa4889f324b4a7e9cff73e84ed7c37d","value":"tokenizer.json: 100%"}},"56b60bad67c4401ab748c58dd67eff25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5533c15b76b3490e9bb969903c8507ce","IPY_MODEL_112fecebc1704c859b625df453613959","IPY_MODEL_7f4a1a896ef842939ffc8f4e333342f5"],"layout":"IPY_MODEL_ecd4270983a8456ea132baa652dd60a4"}},"5721f08daeeb487d8adf5d146bb9683a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58b7c91b6b3a47df843da10f727a46b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ded4b76625d243a8b979400d0ef7a06f","IPY_MODEL_842f4c0a54d64ecab98e335bcdeceb87","IPY_MODEL_6f14d7dcb6fa4e51ae5479928aa99bae"],"layout":"IPY_MODEL_73171aa6fbbc416c86a06deda71bb709"}},"5bc3a78725a443fbade05c8edfbec056":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3bbac326ed4a4b93a52adaf1adaded10","IPY_MODEL_c60f9d4ab5814195885e290e80b12e28","IPY_MODEL_3984a8c76c39459eae5e7daf3e1ec2e8"],"layout":"IPY_MODEL_bf8e192732804a28bb66276995a1b8a8"}},"61fdca9a42f34a9698f18d6a408207b8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f14d7dcb6fa4e51ae5479928aa99bae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61fdca9a42f34a9698f18d6a408207b8","placeholder":"​","style":"IPY_MODEL_527aed383aa447b0954c9f2d9afce42e","value":" 1.26k/1.26k [00:00&lt;00:00, 92.7kB/s]"}},"73171aa6fbbc416c86a06deda71bb709":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c230e2834804a8eafa1f2812212f786":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7da557a6bce646a896e590bc7df89736":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dfdf16ced044e2fb391d9537ed37b84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7eff68d1955645aebe35481d414610e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f4a1a896ef842939ffc8f4e333342f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c70c8da9cd704c42b35d372315ca33f6","placeholder":"​","style":"IPY_MODEL_2a98b6e3d4a34fc6b87c608e6fd30951","value":" 7.03M/7.03M [00:00&lt;00:00, 24.0MB/s]"}},"81b1b70022f94a88865eb1952049a3b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"842f4c0a54d64ecab98e335bcdeceb87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5533b8861dde41e0b9dfd8430c3f81f7","max":1262,"min":0,"orientation":"horizontal","style":"IPY_MODEL_006c3f3672dd4a9d8d32cacf4a7acd30","value":1262}},"89a06c3b12f4486fa4aaf68c64cec930":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f8ea3c2c43440ba963f8c7278ecb5c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9aa4889f324b4a7e9cff73e84ed7c37d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0e52f7a5df94245b351931c29377e26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a25ffb12264d477dae13999651d9d086":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4fcebceac44406f98510f3eabef603f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc9f392f42e04970a4ab6cdeb0159258","placeholder":"​","style":"IPY_MODEL_ef2d50f530834923bc4049af5f2449c6","value":" 1.67M/1.67M [00:00&lt;00:00, 6.98MB/s]"}},"a9cfa1095dd64c369985f659cb46797a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd8bba82c60c4bee99364af438e44b88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be369ccb0b1d444e898e09120a83423e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf8e192732804a28bb66276995a1b8a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c60f9d4ab5814195885e290e80b12e28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc3dbdf8a5794c0eae48a120d83adde3","max":7305,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8f8ea3c2c43440ba963f8c7278ecb5c1","value":7305}},"c70c8da9cd704c42b35d372315ca33f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc3dbdf8a5794c0eae48a120d83adde3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc9f392f42e04970a4ab6cdeb0159258":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd9d54afee4c4472915585c544b2d281":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7eff68d1955645aebe35481d414610e2","placeholder":"​","style":"IPY_MODEL_d57d5eb6e4204cacac77d18ec63b7a6b","value":"merges.txt: 100%"}},"d57d5eb6e4204cacac77d18ec63b7a6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8baa8b0a3bd4d24b17e5ec0e7f92181":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5721f08daeeb487d8adf5d146bb9683a","max":2776833,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2768df3fa7f7455bbbb5739c589364e9","value":2776833}},"da060c8909c74be58f1317ce36791e4e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db83b3ef968c4533b330724fd89094dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cd9d54afee4c4472915585c544b2d281","IPY_MODEL_0a6f4784a699413aa69085ab65f51e85","IPY_MODEL_a4fcebceac44406f98510f3eabef603f"],"layout":"IPY_MODEL_2f4da549b3124f27855d69f86c17246e"}},"ded4b76625d243a8b979400d0ef7a06f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89a06c3b12f4486fa4aaf68c64cec930","placeholder":"​","style":"IPY_MODEL_0031255ec1ae417fa62c0b557a17919b","value":"config.json: 100%"}},"e0f921f9f57f48a6a9417f2e5463ed3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4df89cbb549c4357aaa911378c1bdaa2","IPY_MODEL_d8baa8b0a3bd4d24b17e5ec0e7f92181","IPY_MODEL_3ca2ab410742422098ee555c9bca210e"],"layout":"IPY_MODEL_da060c8909c74be58f1317ce36791e4e"}},"e4e5d6a45d484737b1eb98ca870c10b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecd4270983a8456ea132baa652dd60a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eeb6831d4f3d48f99c86977a9395b71e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef2d50f530834923bc4049af5f2449c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"cc8e099a-0952-4fcd-b112-6292c30b73e0","cell_type":"markdown","source":"# Reference \n## Our code motivated from Ashrafur Rahman  https://www.kaggle.com/code/risenfromashes/workshop-2-tir ","metadata":{"papermill":{"duration":0.029708,"end_time":"2024-10-31T15:07:27.027703","exception":false,"start_time":"2024-10-31T15:07:26.997995","status":"completed"},"tags":[]}},{"id":"9b635190-ed85-4142-9cbd-d80719421a38","cell_type":"markdown","source":"# Install vllm","metadata":{}},{"id":"9d68a4a9-8f13-44b8-a4bf-4cc89aa1e93d","cell_type":"code","source":"%%time\n!pip uninstall -y torch\n!pip install -U --no-index --find-links=/kaggle/input/vllm-whl -U vllm\n!pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:15:12.990096Z","iopub.execute_input":"2024-11-01T17:15:12.990949Z","iopub.status.idle":"2024-11-01T17:17:47.553924Z","shell.execute_reply.started":"2024-11-01T17:15:12.990900Z","shell.execute_reply":"2024-11-01T17:17:47.552761Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.4.0\nUninstalling torch-2.4.0:\n  Successfully uninstalled torch-2.4.0\nLooking in links: /kaggle/input/vllm-whl\nProcessing /kaggle/input/vllm-whl/vllm-0.4.0.post1-cp310-cp310-manylinux1_x86_64.whl\nProcessing /kaggle/input/vllm-whl/cmake-3.29.0.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from vllm) (1.11.1.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from vllm) (5.9.3)\nRequirement already satisfied: ray>=2.9 in /opt/conda/lib/python3.10/site-packages (from vllm) (2.24.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from vllm) (0.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from vllm) (1.26.4)\nProcessing /kaggle/input/vllm-whl/torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (from vllm)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from vllm) (2.32.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from vllm) (9.0.0)\nRequirement already satisfied: transformers>=4.39.1 in /opt/conda/lib/python3.10/site-packages (from vllm) (4.45.1)\nProcessing /kaggle/input/vllm-whl/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (from vllm)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from vllm) (0.111.0)\nRequirement already satisfied: uvicorn[standard] in /opt/conda/lib/python3.10/site-packages (from vllm) (0.30.1)\nRequirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from vllm) (2.9.2)\nRequirement already satisfied: prometheus-client>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from vllm) (0.20.0)\nProcessing /kaggle/input/vllm-whl/pynvml-11.5.0-py3-none-any.whl (from vllm)\nProcessing /kaggle/input/vllm-whl/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from vllm)\nProcessing /kaggle/input/vllm-whl/outlines-0.0.34-py3-none-any.whl (from vllm)\nProcessing /kaggle/input/vllm-whl/tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nProcessing /kaggle/input/vllm-whl/interegular-0.3.3-py37-none-any.whl (from outlines==0.0.34->vllm)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (3.1.4)\nProcessing /kaggle/input/vllm-whl/lark-1.1.9-py3-none-any.whl (from outlines==0.0.34->vllm)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (1.6.0)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (3.0.0)\nProcessing /kaggle/input/vllm-whl/diskcache-5.6.3-py3-none-any.whl (from outlines==0.0.34->vllm)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (1.14.1)\nRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (0.60.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (1.4.2)\nRequirement already satisfied: referencing in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (0.35.1)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (4.22.0)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken==0.6.0->vllm) (2024.5.15)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (3.15.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (3.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (2024.6.1)\nProcessing /kaggle/input/vllm-whl/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->vllm)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->vllm) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->vllm) (2.23.4)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (8.1.7)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (1.0.8)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (21.3)\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (3.20.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (6.0.2)\nRequirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (1.3.1)\nRequirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (1.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->vllm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->vllm) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->vllm) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->vllm) (2024.8.30)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.1->vllm) (0.25.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.1->vllm) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.1->vllm) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.1->vllm) (4.66.4)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (0.37.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (0.0.4)\nRequirement already satisfied: httpx>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (0.27.0)\nRequirement already satisfied: python-multipart>=0.0.7 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (0.0.9)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (5.10.0)\nRequirement already satisfied: orjson>=3.2.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (3.10.4)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (2.1.1)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.22.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (12.0)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->vllm) (2.6.1)\nRequirement already satisfied: typer>=0.12.3 in /opt/conda/lib/python3.10/site-packages (from fastapi-cli>=0.0.2->fastapi->vllm) (0.12.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi->vllm) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi->vllm) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi->vllm) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->outlines==0.0.34->vllm) (2.1.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ray>=2.9->vllm) (3.1.2)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->outlines==0.0.34->vllm) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->outlines==0.0.34->vllm) (2023.12.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->outlines==0.0.34->vllm) (0.18.1)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->outlines==0.0.34->vllm) (0.43.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.2->vllm) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.23.0->fastapi->vllm) (1.2.0)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (13.7.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (0.1.2)\nInstalling collected packages: triton, pynvml, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lark, interegular, diskcache, cmake, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers, outlines, vllm\n  Attempting uninstall: pynvml\n    Found existing installation: pynvml 11.4.1\n    Uninstalling pynvml-11.4.1:\n      Successfully uninstalled pynvml-11.4.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nucxx 0.39.1 requires libucx>=1.15.0, which is not installed.\ndask-cuda 24.8.2 requires pynvml<11.5,>=11.0.0, but you have pynvml 11.5.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cmake-3.29.0.1 diskcache-5.6.3 interegular-0.3.3 lark-1.1.9 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 outlines-0.0.34 pynvml-11.5.0 tiktoken-0.6.0 torch-2.1.2 triton-2.1.0 vllm-0.4.0.post1 xformers-0.0.23.post1\nProcessing /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\ngrpcio is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nProcessing /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (8.1.7)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (3.15.1)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (4.22.0)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (1.0.8)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (21.3)\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (3.20.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (6.0.2)\nRequirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (1.3.1)\nRequirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (1.4.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (2.32.3)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.11.0) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.11.0) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.11.0) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.11.0) (0.18.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ray==2.11.0) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.11.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.11.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.11.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.11.0) (2024.8.30)\nInstalling collected packages: ray\n  Attempting uninstall: ray\n    Found existing installation: ray 2.24.0\n    Uninstalling ray-2.24.0:\n      Successfully uninstalled ray-2.24.0\nSuccessfully installed ray-2.11.0\nCPU times: user 1.83 s, sys: 427 ms, total: 2.26 s\nWall time: 2min 34s\n","output_type":"stream"}],"execution_count":1},{"id":"da25c9f6-936c-4af9-9720-02010c5cae49","cell_type":"markdown","source":"# Imports ALL Library ","metadata":{"papermill":{"duration":0.033663,"end_time":"2024-10-31T15:09:59.778299","exception":false,"start_time":"2024-10-31T15:09:59.744636","status":"completed"},"tags":[]}},{"id":"4c14a1e9-6c6f-46e7-a958-c5c8376c1d3c","cell_type":"code","source":"import vllm\n\nimport re\n\nimport csv\n\nimport torch\n\nimport gc\n\nfrom tqdm import tqdm\n\nimport pandas as pd\n\nfrom queue import Queue, Empty\n\nimport signal\n\nimport subprocess\n\nimport tempfile\n\nfrom collections import Counter\n\nfrom contextlib import contextmanager\n\nimport os, math, numpy as np\n\nimport threading\n\nfrom concurrent.futures import ThreadPoolExecutor, as_completed","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:17:47.556293Z","iopub.execute_input":"2024-11-01T17:17:47.557114Z","iopub.status.idle":"2024-11-01T17:17:52.068050Z","shell.execute_reply.started":"2024-11-01T17:17:47.557055Z","shell.execute_reply":"2024-11-01T17:17:52.067275Z"},"papermill":{"duration":4.69316,"end_time":"2024-10-31T15:10:04.505684","exception":false,"start_time":"2024-10-31T15:09:59.812524","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"2024-11-01 17:17:51,173\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n","output_type":"stream"}],"execution_count":2},{"id":"3cb48440-0092-4a8c-be75-9424c455bcb0","cell_type":"markdown","source":"# Set the visible GPUs for CUDA operations","metadata":{}},{"id":"35a9e484-a291-4795-a96e-882290480bed","cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:17:52.069109Z","iopub.execute_input":"2024-11-01T17:17:52.069556Z","iopub.status.idle":"2024-11-01T17:17:52.073912Z","shell.execute_reply.started":"2024-11-01T17:17:52.069523Z","shell.execute_reply":"2024-11-01T17:17:52.072887Z"},"trusted":true},"outputs":[],"execution_count":3},{"id":"396c46cc-e977-4d8b-ae6d-aab68b169f20","cell_type":"markdown","source":"# Methods for identifying, executing, and managing Python code blocks within text","metadata":{}},{"id":"d27daab4-ee56-4e5e-834a-d9ddac33a0db","cell_type":"markdown","source":"## Find Python code blocks within text","metadata":{"papermill":{"duration":0.034288,"end_time":"2024-10-31T15:10:04.576121","exception":false,"start_time":"2024-10-31T15:10:04.541833","status":"completed"},"tags":[]}},{"id":"4534989a-7c31-4708-a70d-790e8fadbef0","cell_type":"code","source":"def find_python_blocks(text):\n\n    blocks = re.findall(r\"(```python.*?```)\", text, re.DOTALL)\n\n    # filter blocks by trying to convert them to float or int\n\n    filtered_blocks = []\n\n    for block in blocks:\n\n        code = block[len(\"```python\"):-len(\"```\")].strip()\n\n        try:\n\n            x = int(code)\n\n        except:\n\n            filtered_blocks.append(code)\n\n            continue\n\n        try:\n\n            x = float(code)\n\n        except:\n\n            filtered_blocks.append(code)\n\n    return filtered_blocks        ","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:17:52.076665Z","iopub.execute_input":"2024-11-01T17:17:52.077270Z","iopub.status.idle":"2024-11-01T17:17:52.091262Z","shell.execute_reply.started":"2024-11-01T17:17:52.077219Z","shell.execute_reply":"2024-11-01T17:17:52.090403Z"},"papermill":{"duration":0.043498,"end_time":"2024-10-31T15:10:04.654130","exception":false,"start_time":"2024-10-31T15:10:04.610632","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"id":"4ea178ac-f0fc-46e8-a251-94078eab718b","cell_type":"markdown","source":"## Class to Execute Python code (adopted from Numina)","metadata":{"papermill":{"duration":0.033862,"end_time":"2024-10-31T15:10:04.722456","exception":false,"start_time":"2024-10-31T15:10:04.688594","status":"completed"},"tags":[]}},{"id":"2c36248b-c73a-40a9-9aa5-a41bea75ef5b","cell_type":"code","source":"class PythonREPL:\n\n    def __init__(self, timeout=5):\n\n        self.timeout = timeout\n\n    # handles timeout\n\n    @contextmanager\n\n    def time_limit(self, seconds):\n\n        def signal_handler(*_):\n\n            raise TimeoutError(f\"Timed out after {seconds} seconds.\")\n\n\n\n        signal.signal(signal.SIGALRM, signal_handler)\n\n        signal.alarm(seconds)\n\n        try:\n\n            yield\n\n        finally:\n\n            signal.alarm(0)\n\n\n\n    def __call__(self, query):\n\n        query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n\n        query = query.strip().split(\"\\n\")\n\n        if \"print(\" not in query[-1]:\n\n            if \"#\" in query[-1]:\n\n                query[-1] = query[-1].split(\"#\")[0]\n\n            query[-1] = \"print(\" + query[-1] + \")\"\n\n        query = \"\\n\".join(query)\n\n        with tempfile.TemporaryDirectory() as temp_dir:\n\n            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n\n            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n\n                f.write(query)\n\n            with self.time_limit(self.timeout):\n\n                result = subprocess.run(\n\n                    [\"python3\", temp_file_path],\n\n                    capture_output=True,\n\n                    check=False,\n\n                    text=True,\n\n                    timeout=self.timeout,\n\n                )\n\n                if result.returncode == 0:\n\n                    output = result.stdout\n\n                    return True, output.strip()\n\n                error_msg = result.stderr.strip()\n\n                msgs = error_msg.split(\"\\n\")\n\n                new_msgs = []\n\n                want_next = False\n\n                for m in msgs:\n\n                    if \"Traceback\" in m:\n\n                        new_msgs.append(m)\n\n                    elif m == msgs[-1]:\n\n                        new_msgs.append(m)\n\n                    elif temp_file_path in m:\n\n                        st = m.index('\"/') + 1 if '\"/' in m else 0\n\n                        ed = m.index(temp_file_path) + 1 if temp_file_path in m else None\n\n                        clr = m[st:ed] if not ed else m[st:]\n\n                        m = m.replace(clr, \"\")\n\n                        new_msgs.append(m)\n\n                        want_next = True\n\n                    elif want_next:\n\n                        new_msgs.append(m)\n\n                        want_next = False\n\n                error_msg = \"\\n\".join(new_msgs)\n\n                return False, error_msg.strip()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:17:52.092486Z","iopub.execute_input":"2024-11-01T17:17:52.092790Z","iopub.status.idle":"2024-11-01T17:17:52.108891Z","shell.execute_reply.started":"2024-11-01T17:17:52.092756Z","shell.execute_reply":"2024-11-01T17:17:52.108030Z"},"papermill":{"duration":0.050775,"end_time":"2024-10-31T15:10:04.807468","exception":false,"start_time":"2024-10-31T15:10:04.756693","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":5},{"id":"90184302-3774-4676-bc6b-103e763cee5a","cell_type":"markdown","source":"## Execute a Python code block","metadata":{"papermill":{"duration":0.034055,"end_time":"2024-10-31T15:10:04.875472","exception":false,"start_time":"2024-10-31T15:10:04.841417","status":"completed"},"tags":[]}},{"id":"293400d9-ff5f-41d3-a807-94e27c8631b7","cell_type":"code","source":"def execute(executor, code):\n\n    success = False\n\n    for lib in (\"subprocess\", \"venv\"):\n\n        if lib in code:\n\n            output = f\"{lib} is not allowed\"\n\n            outputs.append(output)\n\n            successes.append(success)\n\n            continue\n\n    try:\n\n        success, output = executor(code)\n\n    except TimeoutError as e:\n\n        output = str(e)\n\n\n\n    output = output.strip()\n\n    \n\n    return output, success","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:17:52.110033Z","iopub.execute_input":"2024-11-01T17:17:52.110744Z","iopub.status.idle":"2024-11-01T17:17:52.123762Z","shell.execute_reply.started":"2024-11-01T17:17:52.110701Z","shell.execute_reply":"2024-11-01T17:17:52.122980Z"},"papermill":{"duration":0.04172,"end_time":"2024-10-31T15:10:04.951773","exception":false,"start_time":"2024-10-31T15:10:04.910053","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":6},{"id":"4e74bd77-3760-4d3e-b722-0ceb1c98c54b","cell_type":"markdown","source":"# Load huggingface_hub","metadata":{"papermill":{"duration":0.033893,"end_time":"2024-10-31T15:10:05.019982","exception":false,"start_time":"2024-10-31T15:10:04.986089","status":"completed"},"tags":[]}},{"id":"b78a04f3-7881-4d32-a419-969a38ba1dfd","cell_type":"code","source":"!pip install huggingface_hub\n\n!git config --global credential.helper store\n\nfrom huggingface_hub import login\n\n\n\n# Log in and save the token to Git credentials\n\nlogin(token=\"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\", add_to_git_credential=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:17:52.125090Z","iopub.execute_input":"2024-11-01T17:17:52.125854Z","iopub.status.idle":"2024-11-01T17:18:05.135734Z","shell.execute_reply.started":"2024-11-01T17:17:52.125810Z","shell.execute_reply":"2024-11-01T17:18:05.134623Z"},"papermill":{"duration":12.848724,"end_time":"2024-10-31T15:10:17.902957","exception":false,"start_time":"2024-10-31T15:10:05.054233","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\nToken is valid (permission: write).\nYour token has been saved in your configured git credential helpers (store).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":7},{"id":"2029340d-6f96-4687-b5e0-c1f1aeb9e370","cell_type":"markdown","source":"# We use the Qwen2.5-32B-Instruct-GPTQ-Int4  Model here. You can explore other models.","metadata":{"papermill":{"duration":0.034512,"end_time":"2024-10-31T15:10:17.973170","exception":false,"start_time":"2024-10-31T15:10:17.938658","status":"completed"},"tags":[]}},{"id":"57454066-abe9-42f3-9d4e-60f860289743","cell_type":"code","source":"# Initialize the LLM with specified parameters\nllm = vllm.LLM(\n    \"Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\",  # Model name and version\n    tensor_parallel_size=2,  # Number of GPUs to use in parallel for tensor operations\n    gpu_memory_utilization=0.95,  # Percentage of GPU memory to utilize\n    trust_remote_code=True,  # Allow execution of code from remote sources\n    enforce_eager=True,  # Enforce eager execution mode for debugging or specific use cases\n    revision=\"main\",  # Specify the model version or branch\n    max_model_len=4096,  # Maximum token length for model inputs\n)\n\n# Get the tokenizer associated with the initialized LLM\ntokenizer = llm.get_tokenizer()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:18:05.137251Z","iopub.execute_input":"2024-11-01T17:18:05.137603Z","iopub.status.idle":"2024-11-01T17:18:49.891365Z","shell.execute_reply.started":"2024-11-01T17:18:05.137568Z","shell.execute_reply":"2024-11-01T17:18:49.838532Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e3577abfab943d9bd03c0635385892d"}},"metadata":{}},{"name":"stdout","text":"WARNING 11-01 17:18:05 config.py:211] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n","output_type":"stream"},{"name":"stderr","text":"2024-11-01 17:18:08,002\tINFO worker.py:1749 -- Started a local Ray instance.\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-01 17:18:09 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4', tokenizer='Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4', tokenizer_mode=auto, revision=main, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"177806c8a77f4e338f5da0b9cd1d9ea4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edeca5734b08400bb54d54a14e0299b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42b4da612791409aaab71a0f1cd1519a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a32b9938ed41bcaa13d1da087baed1"}},"metadata":{}},{"name":"stdout","text":"INFO 11-01 17:18:19 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\nINFO 11-01 17:18:19 selector.py:25] Using XFormers backend.\n\u001b[36m(RayWorkerVllm pid=410)\u001b[0m INFO 11-01 17:18:20 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\n\u001b[36m(RayWorkerVllm pid=410)\u001b[0m INFO 11-01 17:18:20 selector.py:25] Using XFormers backend.\nINFO 11-01 17:18:20 pynccl_utils.py:45] vLLM is using nccl==2.18.1\n\u001b[36m(RayWorkerVllm pid=410)\u001b[0m INFO 11-01 17:18:20 pynccl_utils.py:45] vLLM is using nccl==2.18.1\nINFO 11-01 17:18:22 weight_utils.py:177] Using model weights format ['*.safetensors']\n\u001b[36m(RayWorkerVllm pid=410)\u001b[0m INFO 11-01 17:18:22 weight_utils.py:177] Using model weights format ['*.safetensors']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbcd0c98accd4df1be15c61ae98dd48b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/3.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9703b61a0313443ab77e0122ca5424d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"751c9c2d2e0d4fccb613212e22857b5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cd2a81d34964a24b2c8e3edda9a6534"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"991e4953a84749adb740b231dcb3f7d0"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                   initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m obj\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1169\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable:\n\u001b[0;32m-> 1169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n","File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n","File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the LLM with specified parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mvllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQwen/Qwen2.5-32B-Instruct-GPTQ-Int4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Model name and version\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of GPUs to use in parallel for tensor operations\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Percentage of GPU memory to utilize\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Allow execution of code from remote sources\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Enforce eager execution mode for debugging or specific use cases\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify the model version or branch\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Maximum token length for model inputs\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Get the tokenizer associated with the initialized LLM\u001b[39;00m\n\u001b[1;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mget_tokenizer()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/entrypoints/llm.py:112\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     94\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m     95\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     96\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    111\u001b[0m )\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/engine/llm_engine.py:196\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    193\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m GPUExecutor\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/engine/llm_engine.py:110\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, vision_language_config, executor_class, log_stats, usage_context)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenizer \u001b[38;5;241m=\u001b[39m Detokenizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mvision_language_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py:62\u001b[0m, in \u001b[0;36mRayGPUExecutor.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, vision_language_config)\u001b[0m\n\u001b[1;32m     59\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAY_USAGE_STATS_ENABLED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Create the parallel GPU workers.\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_workers_ray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplacement_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Profile the memory usage and initialize the cache.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_cache()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py:192\u001b[0m, in \u001b[0;36mRayGPUExecutor._init_workers_ray\u001b[0;34m(self, placement_group, **ray_remote_kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m Worker(\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config,\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m     is_driver_worker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_device\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_concurrent_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_parallel_loading_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py:324\u001b[0m, in \u001b[0;36mRayGPUExecutor._run_workers\u001b[0;34m(self, method, driver_args, driver_kwargs, max_concurrent_workers, use_ray_compiled_dag, *args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     driver_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# Start the driver worker after all the ray workers.\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# Get the results of the ray workers.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/worker/worker.py:107\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/worker/model_runner.py:95\u001b[0m, in \u001b[0;36mModelRunner.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CudaMemoryProfiler() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[0;32m---> 95\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvision_language_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_language_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mconsumed_memory\n\u001b[1;32m    104\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights took \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/model_executor/model_loader.py:101\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_config, device_config, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m         initialize_dummy_weights(model)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# Load the weights from the cached or downloaded files.\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:349\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.load_weights\u001b[0;34m(self, model_name_or_path, cache_dir, load_format, revision)\u001b[0m\n\u001b[1;32m    340\u001b[0m stacked_params_mapping \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# (param_name, shard_name, shard_id)\u001b[39;00m\n\u001b[1;32m    342\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqkv_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgate_up_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mup_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    347\u001b[0m ]\n\u001b[1;32m    348\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_parameters(remove_duplicate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, loaded_weight \u001b[38;5;129;01min\u001b[39;00m hf_model_weights_iterator(\n\u001b[1;32m    350\u001b[0m         model_name_or_path, cache_dir, load_format, revision):\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotary_emb.inv_freq\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py:224\u001b[0m, in \u001b[0;36mhf_model_weights_iterator\u001b[0;34m(model_name_or_path, cache_dir, load_format, revision, fall_back_to_pt)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhf_model_weights_iterator\u001b[39m(\n\u001b[1;32m    218\u001b[0m     model_name_or_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    219\u001b[0m     cache_dir: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     fall_back_to_pt: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    223\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m--> 224\u001b[0m     hf_folder, hf_weights_files, use_safetensors \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_hf_model_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfall_back_to_pt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfall_back_to_pt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m load_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpcache\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;66;03m# Currently np_cache only support *.bin checkpoints\u001b[39;00m\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m use_safetensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py:181\u001b[0m, in \u001b[0;36mprepare_hf_model_weights\u001b[0;34m(model_name_or_path, cache_dir, load_format, fall_back_to_pt, revision)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# Use file lock to prevent multiple processes from\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# downloading the same model weights at the same time.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_lock(model_name_or_path, cache_dir):\n\u001b[0;32m--> 181\u001b[0m         hf_folder \u001b[38;5;241m=\u001b[39m \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDisabledtqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     hf_folder \u001b[38;5;241m=\u001b[39m model_name_or_path\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/_snapshot_download.py:290\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[0m\n\u001b[1;32m    288\u001b[0m         _inner_hf_hub_download(file)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFetching \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(local_dir))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/contrib/concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/contrib/concurrent.py:49\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m lock_name \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlock_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                       initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(tqdm_class(ex\u001b[38;5;241m.\u001b[39mmap(fn, \u001b[38;5;241m*\u001b[39miterables, chunksize\u001b[38;5;241m=\u001b[39mchunksize), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n","File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"id":"7f3c6565-fb7b-430f-b59c-db703f55b444","cell_type":"markdown","source":"## Necessary Configuration","metadata":{}},{"id":"c78a6d0f-1113-4380-a45e-2eb08c1dcdc2","cell_type":"code","source":"# DEBUG mode\nDEBUG = False  # Set to True to enable debug logging and diagnostics\n\n# Number of candidate solutions to generate\nK = 16  # The number of candidate solutions generated in each iteration\n\n# Search depth for each candidate solution\nDEPTH = 6  # Depth of the search for candidate solutions\n\n# Temperature for randomness control in a generation\nTEMPERATURE = 0.8  # Controls randomness in sampling (higher = more randomness)\n\n# Top-p sampling threshold for diversity\nTOP_P = 0.9  # Probability threshold for nucleus sampling\n\n# Batch size for processing\nBATCH_SIZE = 64  # Number of samples processed per batch\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:18:49.893086Z","iopub.status.idle":"2024-11-01T17:18:49.893588Z","shell.execute_reply.started":"2024-11-01T17:18:49.893369Z","shell.execute_reply":"2024-11-01T17:18:49.893393Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"bc7a0b37-03a2-43d9-8258-7efb422b2a89","cell_type":"markdown","source":"## Extract boxed answer","metadata":{"papermill":{"duration":0.043316,"end_time":"2024-10-31T15:13:25.836867","exception":false,"start_time":"2024-10-31T15:13:25.793551","status":"completed"},"tags":[]}},{"id":"e1d5430d-3242-44f6-95fb-f45404b7906d","cell_type":"code","source":"def extract_answer(text):\n\n    # find right most boxed answer\n\n    def last_boxed_only_string(text):\n\n        idx = text.rfind(\"\\\\boxed\")\n\n        if idx < 0:\n\n            idx = text.rfind(\"\\\\fbox\")\n\n            if idx < 0:\n\n                return None\n\n        i = idx\n\n        right_brace_idx = None\n\n        num_left_braces_open = 0\n\n        while i < len(text):\n\n            if text[i] == \"{\":\n\n                num_left_braces_open += 1\n\n            if text[i] == \"}\":\n\n                num_left_braces_open -= 1\n\n                if num_left_braces_open == 0:\n\n                    right_brace_idx = i\n\n                    break\n\n            i += 1\n\n        if right_brace_idx is None:\n\n            return None\n\n        return text[idx : right_brace_idx + 1]\n\n    # get content of boxed\n\n    def remove_boxed(boxed):\n\n        left = \"\\\\boxed{\"\n\n        try:\n\n            assert boxed[: len(left)] == left\n\n            assert boxed[-1] == \"}\"\n\n            length = len(left)\n\n            return boxed[length:-1]\n\n        except Exception:\n\n            return None\n\n\n\n    boxed = last_boxed_only_string(text)\n\n    if boxed is None:\n\n        return None\n\n    answer = remove_boxed(boxed)\n\n    return answer","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:18:49.895750Z","iopub.status.idle":"2024-11-01T17:18:49.896203Z","shell.execute_reply.started":"2024-11-01T17:18:49.895973Z","shell.execute_reply":"2024-11-01T17:18:49.895993Z"},"papermill":{"duration":0.056893,"end_time":"2024-10-31T15:13:26.016381","exception":false,"start_time":"2024-10-31T15:13:25.959488","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"e077a1b9-03f9-473c-b7d9-5bd2ce32b934","cell_type":"markdown","source":"## Majority vote (select the most occuring answer)","metadata":{"papermill":{"duration":0.040802,"end_time":"2024-10-31T15:13:26.097855","exception":false,"start_time":"2024-10-31T15:13:26.057053","status":"completed"},"tags":[]}},{"id":"b1dacf4a-8c24-40f7-8cdc-8a0296ff7c97","cell_type":"code","source":"# Define the majority voting function to get the most common answer\n\ndef majority_vote(answers):\n\n    answers = [answer for answer in answers if answer is not None]\n\n\n\n    if not answers:\n\n        return None\n\n    # count the occurence of each answer\n\n    counts = {}\n\n    for answer in answers:\n\n        if answer in counts:\n\n            counts[answer] += 1\n\n        else:\n\n            counts[answer] = 1\n\n\n\n    max_answer = None\n\n    max_count = 0\n\n    \n\n    for answer, count in counts.items():\n\n        if count > max_count:\n\n            max_answer = answer\n\n            max_count = count\n\n    \n\n    return max_answer","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:18:49.900490Z","iopub.status.idle":"2024-11-01T17:18:49.901180Z","shell.execute_reply.started":"2024-11-01T17:18:49.900954Z","shell.execute_reply":"2024-11-01T17:18:49.900977Z"},"papermill":{"duration":0.055385,"end_time":"2024-10-31T15:13:26.199322","exception":false,"start_time":"2024-10-31T15:13:26.143937","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"27cae454-eab2-4627-af3e-aaa717ff16ac","cell_type":"markdown","source":"# TIR Agent","metadata":{"papermill":{"duration":0.036971,"end_time":"2024-10-31T15:13:26.273672","exception":false,"start_time":"2024-10-31T15:13:26.236701","status":"completed"},"tags":[]}},{"id":"447d5ca2-fba4-4dd9-abc7-bcfcb59f54de","cell_type":"code","source":"class TIRAgent:\n\n    def __init__(self, problem_id, id, problem, tokenizer, max_depth, log):\n\n        # problem id\n\n        self.problem_id = problem_id\n\n        # id of the agent\n\n        self.id = id\n\n        # number of LLM turns\n\n        self.depth = 1\n\n        # maximum number of turns allowed\n\n        self.max_depth = max_depth\n\n        # LLM's tokenizer\n\n        self.tokenizer = tokenizer\n\n        # Problem statement\n\n        self.problem = problem\n\n        # Chat Messages\n\n        self.messages = [\n\n            {\n\n                \"role\": \"user\", \n\n                \"content\": f\"\"\"Here is a math problem:\n\n{self.problem}\n\nTo accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\n\nWrite the entire script covering all the steps (use comments and document it well) and print the result. \n\nAfter solving the problem, Put your final integer answer within \\\\boxed{{}}.\"\"\"\n\n            }\n\n        ]\n\n        # Last response from the LLM\n\n        self.last_response = None\n\n        # Code blocks from the last response\n\n        self.blocks = []\n\n        # Answers that the LLM generated in \\boxed{}\n\n        self.answers = []\n\n        # No python code generated in last response or max_depth reached\n\n        self.is_complete = False\n\n        # File to log answers\n\n        self.log = log\n\n        # Next prompt to the LLM\n\n        self.next_prompt = None\n\n        \n\n    def complete(self):\n\n        # is the Agent done\n\n        return self.is_complete\n\n    \n\n    def add_response(self, response, executor):\n\n        self.depth += 1\n\n        # remember this response\n\n        self.last_response = response\n\n        # add this to the messages history\n\n        self.messages.append({\"role\": \"assistant\", \"content\": response})\n\n        # extract python blocks\n\n        self.blocks = find_python_blocks(response)\n\n        # extract answer from the generated text, if present\n\n        answer = extract_answer(response)\n\n        if answer is not None:\n\n            self.answers.append(answer)\n\n        # is it done?\n\n        self.is_complete = not self._should_continue()\n\n        # if not, use the python executor to create next prompt\n\n        if not self.is_complete:\n\n            self.next_prompt = self._next_prompt(executor)   \n\n            self.messages.append({\"role\": \"user\", \"content\": self.next_prompt})\n\n    \n\n    def _should_continue(self):        \n\n        # quit if max_depth number of turns reached\n\n        if self.depth >= self.max_depth:\n\n            return False\n\n        # if no python code generated, we can stop now\n\n        elif len(self.blocks) > 0:\n\n            return True\n\n        return False\n\n    \n\n    def _next_prompt(self, executor):\n\n        assert not self.is_complete\n\n        assert len(self.blocks) > 0\n\n        # get code result from python execution\n\n        output, status = execute(executor, self.blocks[-1])\n\n        \n\n        prompt = ''\n\n        # if code succeeds give the output\n\n        if status:\n\n            prompt = f\"\"\"The python code you provided gives the following output:\n\n```python\n\n{self.blocks[-1]}\n\n```\n\n```output\n\n{output}\n\n```\"\"\"\n\n        # if code fails, give the error\n\n        else:\n\n            prompt = f\"\"\"The python code you provided gives the following error:\n\n```python\n\n{self.blocks[-1]}\n\n```\n\n```output\n\n{output}\n\n```\"\"\"\n\n        return prompt\n\n    \n\n    \n\n    def next_message(self):\n\n        assert not self.is_complete \n\n        # apply chat template to get the text\n\n        text = self.tokenizer.apply_chat_template(\n\n            self.messages,\n\n            tokenize=False,\n\n            add_generation_prompt=True\n\n        )\n\n        \n\n        return text\n\n        \n\n    \n\n    def final_answer(self):\n\n        # if there no answers yet, we have to return None\n\n        ans = None\n\n        # otherwise return the latest answer\n\n        if len(self.answers) > 0:\n\n            ans = self.answers[-1]\n\n        # log to file\n\n        if self.log:\n\n            self.log.writerow([self.problem_id, self.id, ans])\n\n        # try to convert to integer\n\n        try:\n\n            ans = int(ans)\n\n        except:\n\n            ans = None\n\n        \n\n        return ans        ","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:18:49.903775Z","iopub.status.idle":"2024-11-01T17:18:49.904223Z","shell.execute_reply.started":"2024-11-01T17:18:49.903965Z","shell.execute_reply":"2024-11-01T17:18:49.903984Z"},"papermill":{"duration":0.131883,"end_time":"2024-10-31T15:13:26.468023","exception":false,"start_time":"2024-10-31T15:13:26.336140","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"d7631bac-14e6-4873-8650-102a4d6680e2","cell_type":"markdown","source":"# Sc-TIR Agent","metadata":{"papermill":{"duration":0.069776,"end_time":"2024-10-31T15:13:26.607651","exception":false,"start_time":"2024-10-31T15:13:26.537875","status":"completed"},"tags":[]}},{"id":"3b58c851-e7d7-49c1-91d1-9cfff60a59d6","cell_type":"code","source":"class SCTIRAgent:\n\n    def __init__(self, problem_id, problem, tokenizer, samples, max_depth, log):\n\n        # problem id\n\n        self.problem_id = problem_id\n\n        # problem statement\n\n        self.problem = problem\n\n        # LLM's tokenizer\n\n        self.tokenizer = tokenizer\n\n        # number of TIRAgents to create\n\n        self.samples = samples\n\n        # maximum number of turns\n\n        self.max_depth = max_depth\n\n        # TIR Agents\n\n        self.agents = [TIRAgent(problem_id, i, problem, tokenizer, max_depth, log) for i in range(samples)]\n\n        # log file\n\n        self.log = log\n\n    \n\n    def complete(self):\n\n        # only complete when all agents are done\n\n        for agent in self.agents:\n\n            if not agent.complete():\n\n                return False\n\n        return True\n\n        \n\n    def get_ready_agents(self):\n\n        # return agents that are not complete yet\n\n        ready_agents = []\n\n        for agent in self.agents:\n\n            if not agent.complete():\n\n                ready_agents.append(agent)\n\n        return ready_agents\n\n    \n\n    def final_answer(self):\n\n        # majority vote agent answers\n\n        assert self.complete()\n\n        answers = [agent.final_answer() for agent in self.agents]\n\n        answer = majority_vote(answers)\n\n        if answer is None:\n\n            return 0\n\n        return answer","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:18:49.905802Z","iopub.status.idle":"2024-11-01T17:18:49.906223Z","shell.execute_reply.started":"2024-11-01T17:18:49.906013Z","shell.execute_reply":"2024-11-01T17:18:49.906034Z"},"papermill":{"duration":0.112193,"end_time":"2024-10-31T15:13:26.786403","exception":false,"start_time":"2024-10-31T15:13:26.674210","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"d306a6a2-974c-469d-9117-aa669ad86ded","cell_type":"markdown","source":"# Load Dataset","metadata":{"papermill":{"duration":0.069454,"end_time":"2024-10-31T15:13:26.925915","exception":false,"start_time":"2024-10-31T15:13:26.856461","status":"completed"},"tags":[]}},{"id":"1a04f57d-f2c1-4347-a8ec-0c60a48a5e82","cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/dlsprint3/test.csv')\n\ntest_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:18:49.908451Z","iopub.status.idle":"2024-11-01T17:18:49.908909Z","shell.execute_reply.started":"2024-11-01T17:18:49.908666Z","shell.execute_reply":"2024-11-01T17:18:49.908698Z"},"papermill":{"duration":0.143921,"end_time":"2024-10-31T15:13:27.139573","exception":false,"start_time":"2024-10-31T15:13:26.995652","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"c7df8f36-0c23-42ce-a008-4b32868cbf67","cell_type":"markdown","source":"## Debug the model","metadata":{"papermill":{"duration":0.073504,"end_time":"2024-10-31T15:13:27.286984","exception":false,"start_time":"2024-10-31T15:13:27.213480","status":"completed"},"tags":[]}},{"id":"65e3a134-d094-4a4f-9e46-b9b640c87234","cell_type":"code","source":"if DEBUG:\n\n    test_df = test_df[:5]\n\n    torch.cuda.empty_cache()\n\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:18:49.912193Z","iopub.status.idle":"2024-11-01T17:18:49.912689Z","shell.execute_reply.started":"2024-11-01T17:18:49.912436Z","shell.execute_reply":"2024-11-01T17:18:49.912462Z"},"papermill":{"duration":0.055621,"end_time":"2024-10-31T15:13:27.383831","exception":false,"start_time":"2024-10-31T15:13:27.328210","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"36874f45-97d7-4bbf-89c8-ff17d45785a9","cell_type":"markdown","source":"# Configure LLM and Python REPL","metadata":{"papermill":{"duration":0.043508,"end_time":"2024-10-31T15:13:27.472545","exception":false,"start_time":"2024-10-31T15:13:27.429037","status":"completed"},"tags":[]}},{"id":"cdd6a797-0db0-49c2-bcea-78cfe5df7b48","cell_type":"code","source":"sampling_params = vllm.SamplingParams(max_tokens=2048, temperature=TEMPERATURE, top_p=TOP_P)\n\nexecutor = PythonREPL()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:18:49.915503Z","iopub.status.idle":"2024-11-01T17:18:49.915860Z","shell.execute_reply.started":"2024-11-01T17:18:49.915686Z","shell.execute_reply":"2024-11-01T17:18:49.915705Z"},"papermill":{"duration":0.055843,"end_time":"2024-10-31T15:13:27.571425","exception":false,"start_time":"2024-10-31T15:13:27.515582","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"765fb723-2507-4708-88e7-1d2ec07efaad","cell_type":"markdown","source":"## SC-TIR Agent","metadata":{"papermill":{"duration":0.037999,"end_time":"2024-10-31T15:13:27.803622","exception":false,"start_time":"2024-10-31T15:13:27.765623","status":"completed"},"tags":[]}},{"id":"71f5e8db-d521-44f8-8c15-50007593f258","cell_type":"code","source":"for row in test_df.values[:2]:\n\n    problem_id = row[0]\n\n    problem = row[1]\n\n    \n\n    agent = SCTIRAgent(problem_id, problem, tokenizer, samples=3, max_depth=4, log=None)\n\n    \n\n    while not agent.complete():\n\n        ready_agents = agent.get_ready_agents()\n\n        texts = [a.next_message() for a in ready_agents]\n\n        # get response from LLM\n\n        responses = llm.generate(texts, sampling_params)\n\n        # pass response to the agents\n\n        for i, ready_agent in enumerate(ready_agents):\n\n            ready_agent.add_response(responses[i].outputs[0].text, executor)\n\n    \n\n    answer = agent.final_answer()\n\n    print(f\"Problem: {problem}\")\n\n    print(f\"Final answer: {answer}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:18:49.917641Z","iopub.status.idle":"2024-11-01T17:18:49.918053Z","shell.execute_reply.started":"2024-11-01T17:18:49.917830Z","shell.execute_reply":"2024-11-01T17:18:49.917849Z"},"papermill":{"duration":86.822332,"end_time":"2024-10-31T15:14:54.664609","exception":false,"start_time":"2024-10-31T15:13:27.842277","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"dd10774c-abc1-4717-8ade-e9cb600cb1a0","cell_type":"markdown","source":"# Create submission","metadata":{"papermill":{"duration":0.039735,"end_time":"2024-10-31T15:14:54.829078","exception":false,"start_time":"2024-10-31T15:14:54.789343","status":"completed"},"tags":[]}},{"id":"16aee94d-39ab-4e09-a700-448947d835f1","cell_type":"code","source":"file = open('submission.csv', 'w', encoding='utf-8')\n\nlog_file = open('log.csv', 'w', encoding='utf-8')\n\nsubmission = csv.writer(file)\n\nlog = csv.writer(log_file)\n\nsubmission.writerow(['ID', 'Answer'])\n\nlog.writerow(['ID', \"Agent ID\", 'Answer'])","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:18:49.924197Z","iopub.status.idle":"2024-11-01T17:18:49.924627Z","shell.execute_reply.started":"2024-11-01T17:18:49.924392Z","shell.execute_reply":"2024-11-01T17:18:49.924419Z"},"papermill":{"duration":0.055217,"end_time":"2024-10-31T15:14:55.003332","exception":false,"start_time":"2024-10-31T15:14:54.948115","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"6e81f8da-c72c-4fa8-9fad-5cd1afed82b9","cell_type":"markdown","source":"## Use a queue to Batch inference","metadata":{"papermill":{"duration":0.040395,"end_time":"2024-10-31T15:14:55.161974","exception":false,"start_time":"2024-10-31T15:14:55.121579","status":"completed"},"tags":[]}},{"id":"952b77ab-0b1e-4c1e-b5f5-a0cd1e20e7b1","cell_type":"code","source":"%%time\n\n\n\nboxed_answers = {}\n\nagents = []\n\n\n\nq = Queue()\n\n\n\niterator = iter(tqdm(test_df.values))\n\n\n\nwhile True:\n\n    for agent in agents:\n\n        if agent.complete():\n\n            boxed_answers[agent.problem_id] = agent.final_answer()\n\n\n\n    agents[:] = list(filter(lambda a: not a.complete(), agents))\n\n\n\n    while q.qsize() < BATCH_SIZE:\n\n        try:\n\n            row = next(iterator)\n\n        except StopIteration:\n\n            break\n\n\n\n        id = row[0]\n\n        problem = row[1]\n\n\n\n        agent = SCTIRAgent(id, problem, tokenizer, K, DEPTH, log)\n\n        \n\n        agents.append(agent)\n\n\n\n        for tir_agent in agent.get_ready_agents():\n\n            q.put_nowait(tir_agent)\n\n            \n\n    if q.empty():\n\n        break\n\n        \n\n    \n\n    ready_agents = []\n\n    texts = []\n\n    for _ in range(BATCH_SIZE):\n\n        try:\n\n            agent = q.get_nowait()\n\n            ready_agents.append(agent)\n\n            texts.append(agent.next_message())\n\n        except:\n\n            break\n\n\n\n    \n\n    responses = llm.generate(texts, sampling_params)\n\n    responses = [response.outputs[0].text for response in responses]\n\n    \n\n    for i in range(len(ready_agents)):\n\n        agent = ready_agents[i]\n\n        response = responses[i]\n\n        agent.add_response(response, executor)\n\n        if not agent.complete():\n\n            q.put_nowait(agent)\n\n   ","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:18:49.928763Z","iopub.status.idle":"2024-11-01T17:18:49.930092Z","shell.execute_reply.started":"2024-11-01T17:18:49.929815Z","shell.execute_reply":"2024-11-01T17:18:49.929844Z"},"papermill":{"duration":26654.853865,"end_time":"2024-10-31T22:39:10.055507","exception":false,"start_time":"2024-10-31T15:14:55.201642","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"41db94cb-1dcf-42a1-8f01-ac6cece02aa6","cell_type":"markdown","source":"## Write to submission file","metadata":{"papermill":{"duration":1.030909,"end_time":"2024-10-31T22:39:12.084404","exception":false,"start_time":"2024-10-31T22:39:11.053495","status":"completed"},"tags":[]}},{"id":"b71fa813-2277-43cf-9c39-6721658f5449","cell_type":"code","source":"for id, answer in boxed_answers.items():\n\n    submission.writerow([id, answer])","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:18:49.932677Z","iopub.status.idle":"2024-11-01T17:18:49.933336Z","shell.execute_reply.started":"2024-11-01T17:18:49.932976Z","shell.execute_reply":"2024-11-01T17:18:49.933039Z"},"papermill":{"duration":1.026326,"end_time":"2024-10-31T22:39:14.070734","exception":false,"start_time":"2024-10-31T22:39:13.044408","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"b8a7c182-b0e2-43d0-affe-466c3b60ab81","cell_type":"markdown","source":"## Close files","metadata":{"papermill":{"duration":1.006017,"end_time":"2024-10-31T22:39:16.042010","exception":false,"start_time":"2024-10-31T22:39:15.035993","status":"completed"},"tags":[]}},{"id":"0a03c8e7-1467-4c14-959c-a4dc6748e3e1","cell_type":"code","source":"   \n\nfile.close()\n\nlog_file.close()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:18:49.935408Z","iopub.status.idle":"2024-11-01T17:18:49.935931Z","shell.execute_reply.started":"2024-11-01T17:18:49.935677Z","shell.execute_reply":"2024-11-01T17:18:49.935702Z"},"papermill":{"duration":1.013113,"end_time":"2024-10-31T22:39:18.051855","exception":false,"start_time":"2024-10-31T22:39:17.038742","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"8417b7ca-e3fe-40ed-a81b-5f120e15e7c9","cell_type":"code","source":"","metadata":{"papermill":{"duration":1.005527,"end_time":"2024-10-31T22:39:20.011110","exception":false,"start_time":"2024-10-31T22:39:19.005583","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}